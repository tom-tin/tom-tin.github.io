---
layout: post
title:  "Mixture-of-Experts"
date:   2025-03-16 11:00:00 +0700
categories: [ai, llms, moe]
---

* [nanoMoE: Mixture-of-Experts (MoE) LLMs from Scratch in PyTorch](https://cameronrwolfe.substack.com/p/nano-moe).
  * An introductory, simple, and functional implementation of MoE LLM pertaining...
* [Chain-of-Experts is the new upgrade for MoE](https://bdtechtalks.substack.com/p/chain-of-experts-is-the-new-upgrade). [Github](https://github.com/ZihanWang314/coe).
  * We propose Chain-of-Experts (CoE), which fundamentally changes sparse Large Language Model (LLM) processing by implementing **sequential communication** between intra-layer experts within Mixture-of-Experts (MoE) models
  * Mixture-of-Experts (MoE) models process information independently in parallel between experts and have high memory requirements. CoE introduces an iterative mechanism enabling experts to "communicate" by processing tokens on top of outputs from other experts.

