---
layout: post
title:  "Mixture-of-Experts"
date:   2025-03-16 11:00:00 +0700
categories: [ai, llms, moe]
---

* [nanoMoE: Mixture-of-Experts (MoE) LLMs from Scratch in PyTorch](https://cameronrwolfe.substack.com/p/nano-moe).
  * An introductory, simple, and functional implementation of MoE LLM pretraining...
.
