---
layout: post
title:  "Parameter-Efficient Finetuning Methods (PEFT)"
date:   2025-02-17 11:00:00 +0700
categories: [ai, llms, finetuning, peft]
---

## Why PEFT?
- We only train a small part of the LLM (while LoRA still needs to train a large number of parameters).

## Some PEFT Techniques
- Prompt Tuning: [paper from Goolge Research](https://arxiv.org/pdf/2104.08691)
  - We attach the prompts into input data and train only these prompts.
  - Advantage:
    - Require less compute resources compared to LoRA.
    - Support the pretrained models learn more efficiently.
    - Easly adapt to LLMs
  - Issues:
    - The prompts can be too simple that don't reflect the input data for learning.
    - Solution: [Adaptive Prompt: Unlocking the Power of Visual Prompt Tuning](https://arxiv.org/abs/2501.18936)  


## Resources
- [Adaptive Prompt: Unlocking the Power of Visual Prompt Tuning](https://arxiv.org/abs/2501.18936)
